{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "<b>Dependencies required:</b>\n",
    "- tensorflow2.x\n",
    "- keras (comes from tensorflow.keras)\n",
    "- numpy\n",
    "- copy\n",
    "- sklearn\n",
    "- matplotlib\n",
    "\n",
    "<b>Setup requirements:</b>\n",
    "- Set up a folder named 'Images' (same directory as this Jupyter Notebook) to store the output images from the model\n",
    "- Set up a folder named 'Caches' (same directory as this Jupyter Notebook) to store the caches from the model\n",
    "- Download the Tiny ImageNet dataset and put it in the same directory as this Jupyter Notebook. Dataset Download Link: https://tiny-imagenet.herokuapp.com/\n",
    "\n",
    "<b>Section descriptions:</b>\n",
    "- <b>Section 1: Generic Initialization</b>\n",
    "    - Run all the code under Section 1. These are the code to define the main functions required for the experiments\n",
    "- <b>Section 2: Tiny ImageNet</b>\n",
    "    - Run the codes here for experiments on the Tiny ImageNet dataset\n",
    "        - Section 2.1: Customize Model\n",
    "            - Allows you to customize the model used for the experiment.\n",
    "        - Section 2.2: Evaluate Model\n",
    "            - Allows you to run code to evaluate performance of various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Generic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenmask(mask):\n",
    "    \"\"\"Returns the nodes of mask flattened\n",
    "    Input:\n",
    "    mask - the mask position, in dictionary form\n",
    "    Output:\n",
    "    maskflatten - the mask position, flattened out in 1D array\n",
    "    \"\"\"\n",
    "    maskflatten = []\n",
    "    for k, v in mask.items():\n",
    "        curmask = v.flatten()\n",
    "        maskflatten = np.hstack([maskflatten, curmask])\n",
    "    return maskflatten\n",
    "\n",
    "def getmask(layers, mask, maskflatten, mask_type = 'min', percentile = 0.2, printactivation = False, dropOne = False):\n",
    "    \"\"\" Updates mask after each training cycle\n",
    "    Inputs:\n",
    "    layers - Predicted node value per layer\n",
    "    mask - current mask\n",
    "    mask_type - type of mask: min, max, random, min_layer, max_layer, random_layer\n",
    "    maskflatten - Flattened masked indices per layer (1 for mask, 0 for no mask)\n",
    "    percentile - percentage of nodes remaining to be masked\n",
    "    printactivation - Boolean. Whether to print the activations per layer\n",
    "    dropOne - Boolean. Whether to drop only one node/filter at a time\n",
    "    \n",
    "    Output:\n",
    "    mask - final masks after masking percentile proportion of remaining nodes\n",
    "    \"\"\"\n",
    "    nodevalues = []\n",
    "    layermeans = {}\n",
    "    \n",
    "    # if only drop one, then percentile is 0\n",
    "    if dropOne:\n",
    "        percentile = 0\n",
    "    \n",
    "    # if only one layer\n",
    "    if(len(mask)==1):\n",
    "        layermeans[0] = np.mean(np.abs(layers), axis = 0).ravel()\n",
    "        nodevalues = np.hstack([nodevalues, layermeans[0]])\n",
    "        if printactivation:\n",
    "            print('Layer activations:', layermeans[0])\n",
    "        \n",
    "    # if more than one layer\n",
    "    else:\n",
    "        for i in range(len(mask)):\n",
    "            layermeans[i] = np.mean(np.abs(layers[i]), axis = 0).ravel()\n",
    "            nodevalues = np.hstack([nodevalues, layermeans[i]])\n",
    "            if printactivation:\n",
    "                print('Layer activations:', layermeans[i])\n",
    "\n",
    "    # remove only those in maskindex\n",
    "    maskflatten = np.ravel(np.where(maskflatten == 1))\n",
    "    \n",
    "    # find out the threshold node/filter value to remove\n",
    "    if len(maskflatten) > 0:\n",
    "        # for max mask\n",
    "        if mask_type == 'max':\n",
    "            sortedvalues = -np.sort(-nodevalues[maskflatten])\n",
    "            index = int((percentile)*len(sortedvalues))\n",
    "            maxindex = sortedvalues[index]\n",
    "            \n",
    "        # for min or % mask\n",
    "        else:\n",
    "            sortedvalues = np.sort(nodevalues[maskflatten])\n",
    "            index = int(percentile*len(sortedvalues))\n",
    "            maxindex = sortedvalues[index]\n",
    "                           \n",
    "    # Calculate the number of nodes to remove\n",
    "    nummask = 0\n",
    "    \n",
    "    for v in mask.values():\n",
    "        nummask += np.sum(v)\n",
    "    \n",
    "    totalnodes = int((percentile)*nummask)\n",
    "    \n",
    "    if dropOne:\n",
    "        totalnodes = 1\n",
    "\n",
    "    # remove at least one node\n",
    "    if (totalnodes == 0):\n",
    "        totalnodes = 1\n",
    "    \n",
    "    # identify the indices to drop for random mask\n",
    "    if mask_type == 'random':\n",
    "        indices = np.random.permutation(maskflatten)\n",
    "        # take only the first totalnodes number of nodes\n",
    "        indices = indices[:totalnodes]\n",
    "        \n",
    "        dropmaskindex = {}\n",
    "        startindex = 0\n",
    "        # assign nodes/filters to drop for each layer in dropmaskindex\n",
    "        for k, v in mask.items():\n",
    "            nummask += np.sum(v)\n",
    "            dropmaskindex[k] = indices[(indices>=startindex) & (indices < startindex + len(v))] - startindex\n",
    "            startindex += len(v)\n",
    "        \n",
    "    for i, layermean in layermeans.items():\n",
    "\n",
    "        #only if there is something to drop in current mask\n",
    "        if(np.sum(mask[i])>0):\n",
    "            # Have different indices for different masks\n",
    "            if mask_type == 'max':\n",
    "                indices = np.ravel(np.where(layermean>=maxindex))\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "                indices = [j for j in indices if j in curindices]\n",
    "            # global random mask or layer random mask\n",
    "            elif mask_type == 'random_layer':\n",
    "                indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "            elif mask_type == 'random':\n",
    "                indices = dropmaskindex[i]\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "            # layer-wise max mask\n",
    "            elif mask_type == 'max_layer':\n",
    "                sortedvalues = -np.sort(-layermean[mask[i]==1])\n",
    "                index = int((percentile)*len(sortedvalues))\n",
    "                maxindex = sortedvalues[index]\n",
    "                indices = np.ravel(np.where(layermean>=maxindex))\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "                indices = [j for j in indices if j in curindices]\n",
    "            # layer-wise min mask\n",
    "            elif mask_type == 'min_layer':\n",
    "                sortedvalues = np.sort(layermean[mask[i]==1])\n",
    "                index = int((percentile)*len(sortedvalues))\n",
    "                maxindex = sortedvalues[index]\n",
    "                indices = np.ravel(np.where(layermean<=maxindex))\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "                indices = [j for j in indices if j in curindices]\n",
    "            # if this is min mask or % based mask\n",
    "            else:\n",
    "                indices = np.ravel(np.where(layermean<=maxindex))\n",
    "                curindices = np.ravel(np.where(mask[i].ravel()))\n",
    "                indices = [j for j in indices if j in curindices]\n",
    "                \n",
    "        else:\n",
    "            #default\n",
    "            indices = np.ravel(np.where(mask[i]==1))\n",
    "\n",
    "        # shuffle the indices only if we are not dropping one node/filter\n",
    "        if (dropOne == False):\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        newmask = mask[i].ravel()\n",
    "\n",
    "        # for layer masks, total nodes dropped is by percentile of the layer of each mask\n",
    "        if(mask_type == 'random_layer') or mask_type == 'min_layer' or mask_type == 'max_layer':\n",
    "            initialpercent = np.sum(mask[i])*1.0/len(mask[i].ravel())\n",
    "            totalnodes = int(initialpercent*(percentile)*len(mask[i].ravel()))\n",
    "\n",
    "            # remove at least 1 node\n",
    "            if (totalnodes == 0):\n",
    "                totalnodes = 1\n",
    "\n",
    "        if(len(indices)>0):\n",
    "\n",
    "            # remove at most totalnodes number of nodes\n",
    "            if(len(indices)>totalnodes):\n",
    "                indices = indices[:totalnodes]\n",
    "\n",
    "            # remove nodes\n",
    "            newmask[indices] = 0\n",
    "\n",
    "            # updated totalnodes to be removed\n",
    "            totalnodes = totalnodes - len(indices)\n",
    "\n",
    "        # reshape to fit new mask\n",
    "        mask[i] = newmask.reshape(mask[i].shape)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def resetmask(mask):\n",
    "    \"\"\"Resets mask to initial start state of all ones\"\"\"\n",
    "    for k, v in mask.items():\n",
    "        mask[k] = np.ones_like(v)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def comparemask(mask1, mask2):\n",
    "    \"\"\" Compares how similar both masks (mask1, mask2) are and returns a percentage similarity \"\"\"\n",
    "    count = 0\n",
    "    totalcount = 0\n",
    "    for k, v in mask1.items():\n",
    "        count += np.sum(mask1[k] == mask2[k])\n",
    "        totalcount += len(mask1[k].ravel())\n",
    "        \n",
    "    return count/totalcount\n",
    "\n",
    "def percentmask(mask):\n",
    "    \"\"\"Returns the percentage of mask that contains 1s\"\"\"\n",
    "    nummask = 0\n",
    "    totalmask = 0\n",
    "    \n",
    "    for v in mask.values():\n",
    "        nummask += np.sum(v)\n",
    "        totalmask += len(v.ravel())\n",
    "        \n",
    "    return nummask/totalmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printgraph(cache, name, numtrials = 15, oracle = False):\n",
    "    \"\"\"Function to print graph\n",
    "    Input: \n",
    "    cache - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "    name - name of model which is run\n",
    "    numtrials - number of experiments conducted\n",
    "    oracle - Boolean. Whether the oraclecomparison graph is required\n",
    "    \n",
    "    Outputs:\n",
    "    Graphs for training, validation, test accuracy, early stopping iteration, oracle comparison (optional)\"\"\"\n",
    "    \n",
    "    # unpack caches\n",
    "    percentremoved, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oraclecomparison = cache\n",
    "    # sort masktypes by alphabetical order\n",
    "    masktypes = sorted(percentremoved.keys())\n",
    "\n",
    "    # Set colors\n",
    "    colors = {}\n",
    "    colors['min'] = 'b'\n",
    "    colors['max'] = 'r'\n",
    "    colors['random'] ='g'\n",
    "    colors['random_layer'] = 'y'\n",
    "    colors['min_layer'] = 'c'\n",
    "    colors['max_layer'] = 'm'\n",
    "    colors['randominit'] = 'y'\n",
    "    colors['oracle'] = 'm'\n",
    "    \n",
    "    # colors for percentage mask\n",
    "    colors['0.1'] = 'b'\n",
    "    colors['0.2'] = 'r'\n",
    "    colors['0.3'] = 'g'\n",
    "    colors['0.4'] = 'y'\n",
    "    colors['0.5'] = 'c'\n",
    "    colors['0.9'] = 'm'\n",
    "    \n",
    "    # format for various masks\n",
    "    fmt = {}\n",
    "    fmt['min'] = '-'\n",
    "    fmt['max'] = '-'\n",
    "    fmt['random'] ='--'\n",
    "    fmt['random_layer'] = '--'\n",
    "    fmt['min_layer'] = '-'\n",
    "    fmt['max_layer'] = '-'\n",
    "    fmt['randominit'] = '--'\n",
    "    fmt['minfast'] = '-'\n",
    "    fmt['oracle'] = '-'\n",
    "    \n",
    "    fmt['0.1'] = '-'\n",
    "    fmt['0.2'] = '-'\n",
    "    fmt['0.3'] = '-'\n",
    "    fmt['0.4'] = '-'\n",
    "    fmt['0.5'] = '-'\n",
    "    fmt['0.9'] = '-'\n",
    "    \n",
    "    import matplotlib.font_manager\n",
    "    from matplotlib import rc, rcParams\n",
    "    rc('font', family = 'STIXGeneral')\n",
    "    rc('xtick', labelsize=10) \n",
    "    rcParams.update({'figure.autolayout': True})\n",
    "    rcParams.update({'font.size': 14})\n",
    "    \n",
    "    # Plot figures for training accuracy\n",
    "    plt.figure()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "    for masktype in masktypes:     \n",
    "        length = len(percentremoved[masktype])//numtrials\n",
    "        mean = []\n",
    "        std = []\n",
    "        for i in range(length):\n",
    "            mean.append(np.mean(train_accuracies[masktype][i::length]))\n",
    "            std.append(1.96*np.std(train_accuracies[masktype][i::length])/np.sqrt(numtrials))\n",
    "        plt.errorbar(percentremoved[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)        \n",
    "    plt.ylabel('Training accuracy')\n",
    "    plt.xlabel('Fraction of filters remaining')\n",
    "    plt.legend(loc = 'lower left')\n",
    "    plt.savefig('Images/training_accuracy_{}.png'.format(name))\n",
    "\n",
    "    # Plot figures for validation accuracy\n",
    "    plt.figure()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "    for masktype in masktypes:\n",
    "        length = len(percentremoved[masktype])//numtrials\n",
    "        mean = []\n",
    "        std = []\n",
    "        for i in range(length):\n",
    "            mean.append(np.mean(valid_accuracies[masktype][i::length]))\n",
    "            std.append(1.96*np.std(valid_accuracies[masktype][i::length])/np.sqrt(numtrials))\n",
    "        plt.errorbar(percentremoved[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "    plt.ylabel('Validation accuracy')\n",
    "    plt.xlabel('Fraction of filters remaining')\n",
    "    plt.legend(loc = 'lower left')\n",
    "    plt.savefig('Images/validation_accuracy_{}.png'.format(name))\n",
    "\n",
    "    # Plot figures for test accuracy\n",
    "    plt.figure()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "    for masktype in masktypes:\n",
    "        length = len(percentremoved[masktype])//numtrials\n",
    "        mean = []\n",
    "        std = []\n",
    "        for i in range(length):\n",
    "            mean.append(np.mean(test_accuracies[masktype][i::length]))\n",
    "            std.append(1.96*np.std(test_accuracies[masktype][i::length])/np.sqrt(numtrials))\n",
    "        plt.errorbar(percentremoved[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "    plt.ylabel('Test accuracy')\n",
    "    plt.xlabel('Fraction of filters remaining')\n",
    "    plt.legend(loc = 'lower left')\n",
    "    plt.savefig('Images/test_accuracy_{}.png'.format(name))\n",
    "\n",
    "    if oracle:\n",
    "        # Plot figures for oracle comparison\n",
    "        plt.figure()\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xscale('log')\n",
    "        plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "        for masktype in masktypes:\n",
    "            length = len(percentremoved[masktype])//numtrials\n",
    "            mean = []\n",
    "            std = []\n",
    "            for i in range(length):\n",
    "                mean.append(np.mean(oraclecomparison[masktype][i::length]))\n",
    "                std.append(1.96*np.std(oraclecomparison[masktype][i::length])/np.sqrt(numtrials))\n",
    "            plt.errorbar(percentremoved[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "        plt.ylabel('Test accuracy')\n",
    "        plt.xlabel('Fraction of filters remaining')\n",
    "        plt.legend(loc = 'lower left')\n",
    "        plt.savefig('Images/oracle_comparison_{}.png'.format(name))\n",
    "\n",
    "    # Plot figures for early stopping iteration\n",
    "    plt.figure()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "    for masktype in masktypes:\n",
    "        length = len(percentremoved[masktype])//numtrials\n",
    "        mean = []\n",
    "        std = []\n",
    "        for i in range(length):\n",
    "            mean.append(np.mean(early_stopping[masktype][i::length]))\n",
    "            std.append(1.96*np.std(early_stopping[masktype][i::length])/np.sqrt(numtrials))\n",
    "        plt.errorbar(percentremoved[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "        plt.ylabel('Early stopping iteration')\n",
    "        plt.xlabel('Fraction of filters remaining')\n",
    "        plt.legend(loc = 'lower left')\n",
    "        plt.savefig('Images/early_stopping_{}.png'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddMask(mask, model, activationarray, layer, initialize = True):\n",
    "    \"\"\"Function to add mask to current layer of nodes\n",
    "    Inputs:\n",
    "    mask - mask which contains either 0 (node/filter dropped) or 1 (node/filter remaining)\n",
    "    model - Keras model, defined using Functional API\n",
    "    activationarray - list of Keras layers of which we care about their activation values\n",
    "    layer - the current layer number\n",
    "    initialize - Boolean. True if we want to reset all the masks to 1\n",
    "    \n",
    "    Output:\n",
    "    model - Updated Keras model with the mask layer\n",
    "    activationarray - Updated list of layers which the activation values are important\n",
    "    layer - the updated layer number count\n",
    "    \"\"\"\n",
    "    # only initialize if this is the first time\n",
    "    if initialize is True:\n",
    "        mask[layer] = np.ones(model.shape[1:])\n",
    "    model = Multiply()([model, tf.ones_like(model)*mask[layer].reshape(model.shape[1:])])\n",
    "    activationarray.append(model)\n",
    "    \n",
    "    # increase layer count for next iteration\n",
    "    layer = layer+1\n",
    "\n",
    "    return model, activationarray, layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Lambda\n",
    "\n",
    "def AddFilterMask(mask, model, activationarray, layer, initialize = True):\n",
    "    \"\"\"Function to add mask to filters in Conv2D\n",
    "    Inputs:\n",
    "    mask - mask which contains either 0 (node/filter dropped) or 1 (node/filter remaining)\n",
    "    model - Keras model, defined using Functional API\n",
    "    activationarray - list of Keras layers of which we care about their activation values\n",
    "    layer - the current layer number\n",
    "    initialize - Boolean. True if we want to reset all the masks to 1\n",
    "    \n",
    "    Output:\n",
    "    model - Updated Keras model with the mask layer\n",
    "    activationarray - Updated list of layers which the activation values are important\n",
    "    layer - the updated layer number count\n",
    "    \"\"\"\n",
    "\n",
    "    model2 = GlobalAveragePooling2D()(model)\n",
    "    \n",
    "    # only initialize if this is the first time\n",
    "    if initialize is True:\n",
    "        mask[layer] = np.ones(model2.shape[1:])\n",
    "    # Multiply the activation with the filters\n",
    "    model2 = Multiply()([model2, tf.ones_like(model2)*mask[layer].reshape(model2.shape[1:])])\n",
    "    activationarray.append(model2)\n",
    "    \n",
    "    # do the multiply for the original filters using broadcasting\n",
    "    model = Multiply()([model, tf.ones_like(model)*mask[layer].reshape(1, 1, model.shape[3])])\n",
    "    \n",
    "    # increase layer count for next iteration\n",
    "    layer = layer+1\n",
    "\n",
    "    return model, activationarray, layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def savefile(cache, name):\n",
    "    \"\"\" Function which saves the cache \"\"\"\n",
    "    with open('Caches/'+name+'.p','wb') as outfile:\n",
    "        pickle.dump(cache, outfile)\n",
    "        \n",
    "def loadfile(name):\n",
    "    \"\"\" Function which loads the cache \"\"\"\n",
    "    with open('Caches/'+name+'.p','rb') as infile:\n",
    "        cache = pickle.load(infile)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: TinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiny_imagenet(path):\n",
    "    \"\"\"\n",
    "    Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
    "    TinyImageNet-200 have the same directory structure, so this can be used\n",
    "    to load any of them.\n",
    "    Note: The output is in NHWC format\n",
    "    Inputs:\n",
    "    - path: String giving path to the directory to load.\n",
    "    Returns: A dictionary with the following entries:\n",
    "    - class_names: A list where class_names[i] is a list of strings giving the\n",
    "      WordNet names for class i in the loaded dataset.\n",
    "    - X_train: (N_tr, 64, 64, 3) array of training images\n",
    "    - y_train: (N_tr,) array of training labels\n",
    "    - X_val: (N_val, 64, 64, 3) array of validation images\n",
    "    - y_val: (N_val,) array of validation labels\n",
    "    - label_to_wnid: dictionary with mapping from integer class label to wnid\n",
    "    \n",
    "    Acknowledgements: Helper code modified from CS231n Stanford Student Project\n",
    "    Dataset Download Link: https://tiny-imagenet.herokuapp.com/\n",
    "    \"\"\"\n",
    "    # First load wnids\n",
    "    with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "        wnids = [x.strip() for x in f]\n",
    "#         print(wnids)\n",
    "\n",
    "    # Map wnids to integer labels\n",
    "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "    label_to_wnid = {v: k for k, v in wnid_to_label.items()}\n",
    "\n",
    "    # Use words.txt to get names for each class\n",
    "    with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "        for wnid, words in wnid_to_words.items():\n",
    "            wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "\n",
    "    print('Loading training data:')\n",
    "\n",
    "    # Next load training data.\n",
    "    X_train, y_train = [], []\n",
    "    for i, wnid in enumerate(wnids):\n",
    "        # To figure out the filenames we need to open the boxes file\n",
    "        boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "        with open(boxes_file, 'r') as f:\n",
    "            filenames = [x.split('\\t')[0] for x in f]\n",
    "        num_images = len(filenames)\n",
    "\n",
    "        X_train_block = np.zeros((num_images, 64, 64, 3), dtype=np.float32)\n",
    "        y_train_block = wnid_to_label[wnid] * \\\n",
    "                        np.ones(num_images, dtype=np.int64)\n",
    "        for j, img_file in enumerate(filenames):\n",
    "            img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "            img = Image.open(img_file).convert(\"RGB\")\n",
    "            X_train_block[j] = img\n",
    "        X_train.append(X_train_block)\n",
    "        y_train.append(y_train_block)\n",
    "\n",
    "    # We need to concatenate all training data\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    print('Training data input shape:', X_train.shape)\n",
    "    print('Training data output shape:', y_train.shape)\n",
    "    \n",
    "    print('Loading validation data:')\n",
    "\n",
    "    # Next load validation data\n",
    "    X_val, y_val = None, None\n",
    "    with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "        img_files = []\n",
    "        val_wnids = []\n",
    "        for line in f:\n",
    "            img_file, wnid = line.split('\\t')[:2]\n",
    "            img_files.append(img_file)\n",
    "            val_wnids.append(wnid)\n",
    "        num_val = len(img_files)\n",
    "        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "        X_val = np.zeros((num_val, 64, 64, 3), dtype=np.float32)\n",
    "        for i, img_file in enumerate(img_files):\n",
    "            img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "            img = Image.open(img_file).convert(\"RGB\")\n",
    "            X_val[i] = img\n",
    "            \n",
    "    print('Test data input shape:', X_val.shape)\n",
    "    print('Test data output shape:', y_val.shape)    \n",
    "    \n",
    "#     mean_image = X_train.mean(axis=0)\n",
    "#     X_train -= mean_image[None]\n",
    "#     X_val -= mean_image[None]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'class_names': class_names,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'label_to_wnid': label_to_wnid,\n",
    "        'wnid_to_words': wnid_to_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data:\n",
      "Training data input shape: (100000, 64, 64, 3)\n",
      "Training data output shape: (100000,)\n",
      "Loading validation data:\n",
      "Test data input shape: (10000, 64, 64, 3)\n",
      "Test data output shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "output = load_tiny_imagenet('tiny-imagenet-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, y_train, x_test, y_test = output['X_train'], output['y_train'], output['X_val'], output['y_val']\n",
    "x_train = x_train.reshape(-1, 64, 64, 3)\n",
    "# print(np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test))\n",
    "# print(np.unique(y_train), np.unique(y_test))\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test.reshape(-1, 64, 64, 3)\n",
    "x_test = x_test/255.0\n",
    "\n",
    "# print(np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test))\n",
    "\n",
    "# split into train and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "# print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generatemodelaugment(opt = SGD(lr = 0.1), loss = 'sparse_categorical_crossentropy',\n",
    "                 metrics = ['accuracy'],\n",
    "                  callbacks = [EarlyStopping(monitor='val_loss', mode = 'min', verbose=0, patience=5)], \n",
    "                    num_epochs = 100, percentile = 0.2, verbose = 0, numtrials = 15, batch_size = 32, printvalue = True, printmask = False, printactivation = False):\n",
    "    \"\"\" Function to evaluate the model against various metrics\n",
    "    Inputs:\n",
    "    opt - Keras optimizer\n",
    "    loss - Keras loss\n",
    "    metrics - Keras metrics\n",
    "    callbacks - Keras callbacks\n",
    "    num_epochs - Number of epochs for each training cycle\n",
    "    percentile - Percentile to drop the nodes\n",
    "    verbose - Keras verbose option\n",
    "    numtrials - Number of different experiments to run, each with different random seed\n",
    "    printvalue  - Boolean. Whether to print the accuracies and losses for each pruning percentage\n",
    "    printmask - Boolean. Whether to print the mask values\n",
    "    printactivation - Boolean. Whether to print the node/filter's activation values\n",
    "    \n",
    "    Output:\n",
    "    caches - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    percentremoved ={}\n",
    "    train_accuracies = {}\n",
    "    valid_accuracies = {}\n",
    "    test_accuracies = {}\n",
    "    train_losses = {}\n",
    "    valid_losses ={}\n",
    "    test_losses = {}\n",
    "    early_stopping = {}\n",
    "    oraclecomparison = {}\n",
    "    masktypes = ['min', 'max', 'random', 'min_layer', 'max_layer', 'random_layer']\n",
    "    \n",
    "    for masktype in masktypes:\n",
    "        percentremoved[masktype] = []\n",
    "        train_accuracies[masktype] = []\n",
    "        valid_accuracies[masktype] = []\n",
    "        test_accuracies[masktype] = []\n",
    "        train_losses[masktype] = []\n",
    "        valid_losses[masktype] = []\n",
    "        test_losses[masktype] = []\n",
    "        early_stopping[masktype] = []\n",
    "        \n",
    "    # do for numtrials number of random seeds\n",
    "    for random_seed in range(numtrials):\n",
    "        print('>>>     Random seed number:', random_seed)\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "        tf.random.set_seed(random_seed)    \n",
    "        \n",
    "        # Initialize the model\n",
    "        mask, model, activationmodel = initializemodel()\n",
    "        # model.summary()\n",
    "        model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "        # save original weights\n",
    "        weights_initial = model.get_weights()\n",
    "    \n",
    "    # do for all mask types\n",
    "        for masktype in masktypes:\n",
    "            print('\\n>>> Currently doing', masktype, 'mask <<<')\n",
    "        \n",
    "            mask=resetmask(mask)\n",
    "            percent = percentmask(mask)\n",
    "            \n",
    "            while percent > 0.1:\n",
    "                # Initialize the model with the new mask\n",
    "                tf.keras.backend.clear_session()\n",
    "                del model\n",
    "                gc.collect()\n",
    "                \n",
    "                np.random.seed(random_seed)\n",
    "                tf.random.set_seed(random_seed)  \n",
    "                \n",
    "                _, model, activationmodel = initializemodel(mask)\n",
    "\n",
    "                #Initialize to original weights\n",
    "                model.set_weights(weights_initial)\n",
    "                model.compile(optimizer = opt, loss = loss, metrics = metrics)  \n",
    "                \n",
    "                datagen = ImageDataGenerator(\n",
    "                rotation_range = 40,\n",
    "                width_shift_range = 0.2,\n",
    "                height_shift_range = 0.2,\n",
    "                zoom_range = 0.2,\n",
    "                shear_range = 0.2,\n",
    "                fill_mode = \"nearest\",\n",
    "                horizontal_flip = True\n",
    "                )\n",
    "                # compute quantities required for featurewise normalization\n",
    "                \n",
    "                # (std, mean, and principal components if ZCA whitening is applied)\n",
    "                datagen.fit(x_train)\n",
    "\n",
    "                history = model.fit(datagen.flow(x_train, y_train, batch_size = batch_size, shuffle = True), shuffle = True, steps_per_epoch=len(x_train)//batch_size, epochs = num_epochs, validation_data = (x_val, y_val), \n",
    "                                    callbacks = callbacks, workers = 16, verbose = verbose)\n",
    "                results = model.evaluate(x_test, y_test, workers = 16, verbose = 0)\n",
    "\n",
    "                percent = percentmask(mask)\n",
    "                train_accuracy = history.history['accuracy'][-1]\n",
    "                valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                test_accuracy = results[1]\n",
    "                train_loss = history.history['loss'][-1]\n",
    "                valid_loss = history.history['val_loss'][-1]\n",
    "                test_loss = results[0]\n",
    "                early = len(history.history['accuracy'])\n",
    "\n",
    "                # Append the values for accuracy and loss\n",
    "                percentremoved[masktype].append(percent)\n",
    "                train_accuracies[masktype].append(train_accuracy)\n",
    "                valid_accuracies[masktype].append(valid_accuracy)\n",
    "                test_accuracies[masktype].append(test_accuracy)\n",
    "                train_losses[masktype].append(train_loss)\n",
    "                valid_losses[masktype].append(valid_loss)\n",
    "                test_losses[masktype].append(test_loss)\n",
    "                early_stopping[masktype].append(early)\n",
    "\n",
    "                if printvalue:\n",
    "                    print('Percentage remaining', percent, end = ' ')\n",
    "                    print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                    if printmask:\n",
    "                        print('Mask:', mask)\n",
    "                    print('Train Acc:', train_accuracy, end = ' ')\n",
    "                    print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                    print('Test Acc:', test_accuracy)\n",
    "                    print('Train Loss:', train_loss, end = ' ')\n",
    "                    print('Val loss:', valid_loss, end = ' ')\n",
    "                    print('Test Loss:', test_loss)\n",
    "                    print('Early stopping iteration:', early)\n",
    "\n",
    "                # Remove nodes for next iteration based on metric\n",
    "                layers = activationmodel.predict(x_train)\n",
    "                maskflatten = flattenmask(mask)\n",
    "                mask = getmask(layers, mask, maskflatten, mask_type = masktype, percentile = percentile, printactivation = printactivation)\n",
    "                \n",
    "        cache = (percentremoved, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oraclecomparison)\n",
    "        printgraph(cache, 'evaluate_resnet_tinyimagenet_'+str(random_seed), numtrials = random_seed+1)\n",
    "\n",
    "    cache = (percentremoved, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oraclecomparison)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 5 random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 5, i + 1)\n",
    "    index = np.random.randint(90000)\n",
    "    \n",
    "    plt.imshow(x_train[index])\n",
    "    plt.show()\n",
    "    print('Classified as: ', output['wnid_to_words'][output['label_to_wnid'][y_train[index]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Customize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Add, Dense, Input, Lambda, Multiply, Flatten, Reshape, Conv2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def initializemodel(mask = None):\n",
    "    \"\"\"Initialize model with a certain mask\"\"\"\n",
    "    activationarray = []\n",
    "    layername = {}\n",
    "    # if no mask specified, start with no mask\n",
    "    layer = 0\n",
    "    if mask is None:\n",
    "        mask = {}\n",
    "        initialize = True\n",
    "    else:\n",
    "        initialize = False\n",
    "    \n",
    "    inputs = Input(shape = [64, 64, 3])\n",
    "    \n",
    "    ## Define your model architecture below\n",
    "    ## For every FC layer (dense layer), follow up with an AddMask line to add the node mask\n",
    "    ## For every Conv Layer, follow up with an AddFilterMask line to add the filter mask\n",
    "    \n",
    "    # ResNet\n",
    "    # [64] x 1, 1/2\n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(inputs)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)  \n",
    "    shortcut = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    # [64, 64] x 2, 1/2\n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Add()([shortcut, model])\n",
    "    \n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Add()([shortcut, model])\n",
    "    shortcut = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    # [128, 128] x 2, 1/2\n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    shortcut, activationarray, layer = AddFilterMask(mask, shortcut, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Add()([shortcut, model])\n",
    "    \n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Add()([shortcut, model])\n",
    "    shortcut = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    # [256, 256] x 2, 1/2\n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    shortcut, activationarray, layer = AddFilterMask(mask, shortcut, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Add()([shortcut, model])\n",
    "    \n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Add()([shortcut, model])\n",
    "    shortcut = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    # [512, 512] x 2, 1/2\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    shortcut, activationarray, layer = AddFilterMask(mask, shortcut, activationarray, layer, initialize = initialize)\n",
    "    shortcut = Add()([shortcut, model])\n",
    "    \n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(shortcut)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Add()([shortcut, model])\n",
    "    \n",
    "    model = GlobalAveragePooling2D()(model)  \n",
    "\n",
    "    # Multiply by mask on nodes (Drop nodes)\n",
    "    out = Dense(200, activation = 'softmax')(model)\n",
    "    \n",
    "    model = Model(inputs = [inputs], outputs = [out])    \n",
    "    activationmodel = Model(inputs = [inputs], outputs = activationarray)\n",
    "    \n",
    "    return mask, model, activationmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense, Input, Lambda, Multiply, Flatten, Reshape, Conv2D, AveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def initializemodel(mask = None):\n",
    "    \"\"\"Initialize model with a certain mask\"\"\"\n",
    "    activationarray = []\n",
    "    layername = {}\n",
    "    # if no mask specified, start with no mask\n",
    "    layer = 0\n",
    "    if mask is None:\n",
    "        mask = {}\n",
    "        initialize = True\n",
    "    else:\n",
    "        initialize = False\n",
    "    \n",
    "    inputs = Input(shape = [64, 64, 3])\n",
    "    \n",
    "    ## Define your model architecture below\n",
    "    ## For every FC layer (dense layer), follow up with an AddMask line to add the node mask\n",
    "    ## For every Conv Layer, follow up with an AddFilterMask line to add the filter mask\n",
    "    \n",
    "    # VGG-19\n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(inputs)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)   \n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "#     model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model, activationarray, layer = AddFilterMask(mask, model, activationarray, layer, initialize = initialize)\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "\n",
    "    # Multiply by mask on nodes (Drop nodes)\n",
    "    model = Dense(1024, activation = 'relu')(model)\n",
    "    \n",
    "    out = Dense(200, activation = 'softmax')(model)\n",
    "    \n",
    "    model = Model(inputs = [inputs], outputs = [out])    \n",
    "    activationmodel = Model(inputs = [inputs], outputs = activationarray)\n",
    "    \n",
    "    return mask, model, activationmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>     Random seed number: 0\n",
      "\n",
      ">>> Currently doing min mask <<<\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 5.1016 - accuracy: 0.0165 - val_loss: 4.7861 - val_accuracy: 0.0423\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 4.6848 - accuracy: 0.0554 - val_loss: 4.5204 - val_accuracy: 0.0844\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 4.3208 - accuracy: 0.0991 - val_loss: 4.0720 - val_accuracy: 0.1317\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 4.0144 - accuracy: 0.1422 - val_loss: 3.8963 - val_accuracy: 0.1685\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 95s 34ms/step - loss: 3.7595 - accuracy: 0.1819 - val_loss: 3.5680 - val_accuracy: 0.2184\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 3.5440 - accuracy: 0.2161 - val_loss: 3.4514 - val_accuracy: 0.2388\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 3.3641 - accuracy: 0.2455 - val_loss: 3.4929 - val_accuracy: 0.2369\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 3.2118 - accuracy: 0.2706 - val_loss: 3.5271 - val_accuracy: 0.2482\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 3.0813 - accuracy: 0.2931 - val_loss: 3.4025 - val_accuracy: 0.2658\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 2.9681 - accuracy: 0.3132 - val_loss: 3.0780 - val_accuracy: 0.3087\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 98s 35ms/step - loss: 2.8601 - accuracy: 0.3317 - val_loss: 3.2169 - val_accuracy: 0.2941\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 94s 34ms/step - loss: 2.7646 - accuracy: 0.3508 - val_loss: 3.1237 - val_accuracy: 0.3085\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 2.6770 - accuracy: 0.3676 - val_loss: 2.9685 - val_accuracy: 0.3341\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 2.6001 - accuracy: 0.3821 - val_loss: 3.2862 - val_accuracy: 0.2985\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 2.5284 - accuracy: 0.3952 - val_loss: 3.0878 - val_accuracy: 0.3240\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 2.4589 - accuracy: 0.4069 - val_loss: 3.2442 - val_accuracy: 0.3044\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 2.3988 - accuracy: 0.4181 - val_loss: 3.3472 - val_accuracy: 0.3200\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 2.3419 - accuracy: 0.4295 - val_loss: 3.1689 - val_accuracy: 0.3402\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 2.2897 - accuracy: 0.4384 - val_loss: 3.4287 - val_accuracy: 0.3121\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 97s 35ms/step - loss: 2.2324 - accuracy: 0.4514 - val_loss: 3.3618 - val_accuracy: 0.3142\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 2.1976 - accuracy: 0.4573 - val_loss: 3.3553 - val_accuracy: 0.3143\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 97s 35ms/step - loss: 2.1512 - accuracy: 0.4669 - val_loss: 3.1876 - val_accuracy: 0.3376\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 97s 35ms/step - loss: 2.1129 - accuracy: 0.4750 - val_loss: 3.3355 - val_accuracy: 0.3289\n",
      "Percentage remaining 1.0 Layer nodes: [64.0, 64.0, 64.0, 64.0, 64.0, 128.0, 128.0, 128.0, 128.0, 128.0, 256.0, 256.0, 256.0, 256.0, 256.0, 512.0, 512.0, 512.0, 512.0, 512.0] Train Acc: 0.47496888041496277 Val Acc: 0.328900009393692 Test Acc: 0.32760000228881836\n",
      "Train Loss: 2.112859010696411 Val loss: 3.3354854583740234 Test Loss: 3.3474838733673096\n",
      "Early stopping iteration: 23\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 95s 34ms/step - loss: 5.1321 - accuracy: 0.0137 - val_loss: 4.8695 - val_accuracy: 0.0319\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 95s 34ms/step - loss: 4.7449 - accuracy: 0.0475 - val_loss: 4.5597 - val_accuracy: 0.0742\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 97s 34ms/step - loss: 4.4118 - accuracy: 0.0864 - val_loss: 4.2235 - val_accuracy: 0.1167\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 98s 35ms/step - loss: 4.1168 - accuracy: 0.1280 - val_loss: 3.8853 - val_accuracy: 0.1617\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 66s 23ms/step - loss: 3.8616 - accuracy: 0.1639 - val_loss: 3.8320 - val_accuracy: 0.1718\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 3.6539 - accuracy: 0.1974 - val_loss: 3.5994 - val_accuracy: 0.2142\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 72s 26ms/step - loss: 3.4822 - accuracy: 0.2255 - val_loss: 3.9875 - val_accuracy: 0.1942\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 66s 23ms/step - loss: 3.3356 - accuracy: 0.2508 - val_loss: 3.3951 - val_accuracy: 0.2503\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 92s 33ms/step - loss: 3.2121 - accuracy: 0.2712 - val_loss: 3.4349 - val_accuracy: 0.2628\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 91s 32ms/step - loss: 3.1011 - accuracy: 0.2909 - val_loss: 3.2507 - val_accuracy: 0.2798\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 93s 33ms/step - loss: 3.0021 - accuracy: 0.3075 - val_loss: 3.3746 - val_accuracy: 0.2702\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 98s 35ms/step - loss: 2.9130 - accuracy: 0.3245 - val_loss: 3.2279 - val_accuracy: 0.2915\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 82s 29ms/step - loss: 2.8403 - accuracy: 0.3365 - val_loss: 3.2868 - val_accuracy: 0.2871\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 97s 35ms/step - loss: 2.7683 - accuracy: 0.3498 - val_loss: 3.1745 - val_accuracy: 0.2966\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 74s 26ms/step - loss: 2.7007 - accuracy: 0.3626 - val_loss: 3.1158 - val_accuracy: 0.3089\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 66s 23ms/step - loss: 2.6499 - accuracy: 0.3715 - val_loss: 3.4025 - val_accuracy: 0.2791\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 95s 34ms/step - loss: 2.5948 - accuracy: 0.3816 - val_loss: 3.1967 - val_accuracy: 0.3158\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 93s 33ms/step - loss: 2.5420 - accuracy: 0.3913 - val_loss: 3.1749 - val_accuracy: 0.3165\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 94s 33ms/step - loss: 2.4957 - accuracy: 0.4013 - val_loss: 3.2762 - val_accuracy: 0.3035\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 89s 32ms/step - loss: 2.4447 - accuracy: 0.4117 - val_loss: 3.3820 - val_accuracy: 0.2973\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 88s 31ms/step - loss: 2.4082 - accuracy: 0.4186 - val_loss: 3.4092 - val_accuracy: 0.3031\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 80s 28ms/step - loss: 2.3702 - accuracy: 0.4243 - val_loss: 3.2441 - val_accuracy: 0.3182\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 71s 25ms/step - loss: 2.3396 - accuracy: 0.4296 - val_loss: 3.3863 - val_accuracy: 0.3070\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 94s 34ms/step - loss: 2.3053 - accuracy: 0.4380 - val_loss: 3.1981 - val_accuracy: 0.3333\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 68s 24ms/step - loss: 2.2865 - accuracy: 0.4408 - val_loss: 3.3993 - val_accuracy: 0.3101\n",
      "Percentage remaining 0.5 Layer nodes: [26.0, 22.0, 2.0, 18.0, 3.0, 78.0, 22.0, 85.0, 60.0, 9.0, 229.0, 58.0, 185.0, 147.0, 15.0, 458.0, 295.0, 359.0, 296.0, 33.0] Train Acc: 0.44084563851356506 Val Acc: 0.3100999891757965 Test Acc: 0.30390000343322754\n",
      "Train Loss: 2.286548376083374 Val loss: 3.3993043899536133 Test Loss: 3.391674518585205\n",
      "Early stopping iteration: 25\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 75s 27ms/step - loss: 5.2168 - accuracy: 0.0083 - val_loss: 5.0975 - val_accuracy: 0.0164\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 93s 33ms/step - loss: 4.9736 - accuracy: 0.0254 - val_loss: 4.7251 - val_accuracy: 0.0545\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 90s 32ms/step - loss: 4.6576 - accuracy: 0.0571 - val_loss: 4.4548 - val_accuracy: 0.0836\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 88s 31ms/step - loss: 4.3897 - accuracy: 0.0898 - val_loss: 4.0929 - val_accuracy: 0.1346\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 97s 35ms/step - loss: 4.1381 - accuracy: 0.1244 - val_loss: 4.0527 - val_accuracy: 0.1443\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 88s 31ms/step - loss: 3.9318 - accuracy: 0.1529 - val_loss: 3.8935 - val_accuracy: 0.1661\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 80s 28ms/step - loss: 3.7605 - accuracy: 0.1816 - val_loss: 3.8115 - val_accuracy: 0.1883\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 106s 38ms/step - loss: 3.6202 - accuracy: 0.2038 - val_loss: 3.6345 - val_accuracy: 0.2078\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 96s 34ms/step - loss: 3.5061 - accuracy: 0.2219 - val_loss: 3.5315 - val_accuracy: 0.2228\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 66s 23ms/step - loss: 3.4053 - accuracy: 0.2388 - val_loss: 3.4757 - val_accuracy: 0.2416\n",
      "Epoch 11/100\n",
      " 322/2812 [==>...........................] - ETA: 55s - loss: 3.3216 - accuracy: 0.2526"
     ]
    }
   ],
   "source": [
    "## Model evaluation to compare all metrics\n",
    "cache = generatemodelaugment(opt = SGD(lr = 0.1), loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'], batch_size = 32, callbacks = [EarlyStopping(monitor='val_loss', mode = 'min', verbose=0, patience=10)], num_epochs = 100, numtrials = 15, percentile = 0.5, verbose = 1, printvalue = True, printmask = False, printactivation = False)\n",
    "\n",
    "## Change your model name accordingly\n",
    "modelname = 'tinyimagenet_resnet18_evaluate'\n",
    "\n",
    "## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "printgraph(cache, modelname, numtrials = 15, oracle = False)\n",
    "savefile(cache, modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
